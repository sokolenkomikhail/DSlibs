{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**micro** Усреднение вычисляет общее количетво FP, FN, TP по всем классам, а затем вычисляет precision, recall и f-меру по этим показателям.\n",
    "\n",
    "**macro** Усреднение вычисляет f-меры для каждого класса и находит их невзвешенное среднее. \n",
    "\n",
    "**weighted** Усреднение вычисляет f-меры для каждого класса и находит их взвешенное среднее. Используется в тех случаях, когда каждому классу нужно присвоить свой вес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost (eXtreme Gradient Boosting)**. Построен на основе обычного градиентного бустинга. Но имеет некоторые различия. В частности само дерево (базовая модель) строится иначе. Вместо стандартных критериев информативности (энтропия Шеннона либо критерия Джини) используется похожесть (Similarity). Несколько иначе рассчитывается прирост информативности. \n",
    "\n",
    "После построения деревьев, к ним применяется \"стрижка\" (Prune). Удаляются разбиения, при которых прирост информативности ниже регуляризационного параметра $\\gamma$. \n",
    "\n",
    "\n",
    "**LightGBM**. Также построен на основе обычного градиентного бустинга. Но обучается значительно быстрее. Увеличение скорости осуществляется за счет нескольких факторов: \n",
    "\n",
    "Рост самого дерева отличается от роста обычного дерева, где разбиения осуществляется одновременно (если для одной из нод не сработал один из критериев остановки) в двух нодах на одной глубине. Разбиение осуществляется только в той ноде, в которой больше ошибка. Возвращается к ноде, в которой не было разбиения, если ошибка на ней будет больше, чем у остальных нод. Тем самым построение дерева занимает меньше времени.\n",
    "\n",
    "Gradient-based One-Side Sampling (GOSS) - алгоритм сэмплирования выборки, при котором сохраняются наблюдения с большими ошибками, и к ним добавляется некоторое количество случайно выбранных наблюдений с маленькими ошибками. Таким образом деревья обучаются не на всей выборке, а на всех наблюдениях с большими ошибками + некоторое количество наблюдений с маленькими ошибками.\n",
    "\n",
    "Exclusive Feature Bundling (EFB) - алгоритм объединяет несколько эксклюзивных признаков в один. Т.к. нередко встречаются разреженные данные, в которых признаки являются взаимоиключающимися, то существует возможность объединить эти признаки в один. В данном признаке один диапазон значений отвечает за один признак, а другой диапазон за другой. Таким образом получается также и сокращение количества признаков.\n",
    "\n",
    "\n",
    "**CatBoost**. За счет симметричного построения дерева, где вопрос дублируется в другую ветку, достигается более высокая скорость предсказания в сравнении с другими бустингами. \n",
    "\n",
    "За одну итерацию строится несколько деревьев, которые обучаются не на всей выборке. При этом модели делают предсказания на тех наблюдениях, которые не попали в обучающую подвыборку для данной модели. И следующие деревья обучаются на разнице между истинными значениями и предсказанными. Таким образом CatBoost более устойчив к переобучению, по сравнению с другими бустингами.\n",
    "\n",
    "Хорошо работает с категориальными признаками и пропущенными значениями за счет собственной обработки признаков."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
